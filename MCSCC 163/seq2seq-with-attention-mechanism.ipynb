{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013567,
     "end_time": "2024-10-28T08:42:01.176847",
     "exception": false,
     "start_time": "2024-10-28T08:42:01.163280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"text-align:center;\"> Seq2Seq task with attention</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012377,
     "end_time": "2024-10-28T08:42:01.203085",
     "exception": false,
     "start_time": "2024-10-28T08:42:01.190708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Sequence-to-sequence (Seq2Seq) learning involves training models that convert sequences from one domain to another. A typical example is translating a sentence from one language, like German, into another language, such as English. In this case, the main objective is to translate German sentences into their English equivalents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012661,
     "end_time": "2024-10-28T08:42:01.228253",
     "exception": false,
     "start_time": "2024-10-28T08:42:01.215592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style='text-align:center;'> <img src='https://www.guru99.com/images/1/111318_0848_seq2seqSequ1.png' alt='diagram'></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012843,
     "end_time": "2024-10-28T08:42:01.254078",
     "exception": false,
     "start_time": "2024-10-28T08:42:01.241235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "An essential enhancement to Seq2Seq models is the **attention mechanism**, which enables the model to focus on specific parts of the input sequence while generating each word of the output. This mechanism simulates the human ability to selectively concentrate on relevant pieces of information. For example, when translating a sentence, attention helps the model pay closer attention to specific words in the input, depending on the word being translated at that moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012572,
     "end_time": "2024-10-28T08:42:01.279146",
     "exception": false,
     "start_time": "2024-10-28T08:42:01.266574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"text-align:center;\"> <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*BLq79DDclwGh_hG61A-2Zg.png\n",
    "\" alt=\"Seq2Seq\"> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012604,
     "end_time": "2024-10-28T08:42:01.304499",
     "exception": false,
     "start_time": "2024-10-28T08:42:01.291895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Due to its effectiveness, attention has become a fundamental part of advanced models like **Transformers**, which rely solely on this mechanism to maintain context across sequences.\n",
    "\n",
    "Seq2Seq models are widely used in natural language processing (NLP) tasks, such as text summarization, speech recognition, and even in modeling biological sequences like DNA. In all of these cases, the input and output are sequences, and the model's job is to generate a new sequence from the given input. Seq2Seq models excel in tasks where structured information needs to be converted into another structured form, making them highly versatile across various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275b9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:10.023155Z",
     "iopub.status.busy": "2024-10-29T02:32:10.022746Z",
     "iopub.status.idle": "2024-10-29T02:32:10.028937Z",
     "shell.execute_reply": "2024-10-29T02:32:10.028004Z",
     "shell.execute_reply.started": "2024-10-29T02:32:10.023118Z"
    },
    "papermill": {
     "duration": 15.663819,
     "end_time": "2024-10-28T08:42:16.980872",
     "exception": false,
     "start_time": "2024-10-28T08:42:01.317053",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from string import punctuation\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector, Layer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:11.444020Z",
     "iopub.status.busy": "2024-10-29T02:32:11.443118Z",
     "iopub.status.idle": "2024-10-29T02:32:11.448446Z",
     "shell.execute_reply": "2024-10-29T02:32:11.447403Z",
     "shell.execute_reply.started": "2024-10-29T02:32:11.443967Z"
    },
    "papermill": {
     "duration": 0.02061,
     "end_time": "2024-10-28T08:42:17.014766",
     "exception": false,
     "start_time": "2024-10-28T08:42:16.994156",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:11.997845Z",
     "iopub.status.busy": "2024-10-29T02:32:11.997484Z",
     "iopub.status.idle": "2024-10-29T02:32:12.465630Z",
     "shell.execute_reply": "2024-10-29T02:32:12.464772Z",
     "shell.execute_reply.started": "2024-10-29T02:32:11.997813Z"
    },
    "papermill": {
     "duration": 0.443975,
     "end_time": "2024-10-28T08:42:17.471943",
     "exception": false,
     "start_time": "2024-10-28T08:42:17.027968",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(r'..\\datasets\\archive (1)/deu.txt', 'rt') as file:\n",
    "    df = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:14.183010Z",
     "iopub.status.busy": "2024-10-29T02:32:14.182642Z",
     "iopub.status.idle": "2024-10-29T02:32:14.188261Z",
     "shell.execute_reply": "2024-10-29T02:32:14.187383Z",
     "shell.execute_reply.started": "2024-10-29T02:32:14.182975Z"
    },
    "papermill": {
     "duration": 0.02088,
     "end_time": "2024-10-28T08:42:17.506034",
     "exception": false,
     "start_time": "2024-10-28T08:42:17.485154",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = [w.translate(str.maketrans('', '',punctuation)) for w in sent]\n",
    "    for i in range(len(sent)):\n",
    "        sent[i] = sent[i].lower()\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:14.637482Z",
     "iopub.status.busy": "2024-10-29T02:32:14.637094Z",
     "iopub.status.idle": "2024-10-29T02:32:14.855012Z",
     "shell.execute_reply": "2024-10-29T02:32:14.854211Z",
     "shell.execute_reply.started": "2024-10-29T02:32:14.637444Z"
    },
    "papermill": {
     "duration": 0.246015,
     "end_time": "2024-10-28T08:42:17.764697",
     "exception": false,
     "start_time": "2024-10-28T08:42:17.518682",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def select(text):\n",
    "    a, b = [], []\n",
    "    for line in text:\n",
    "        line = line.split('\\t')\n",
    "        a.append(line[0]) ; b.append(line[1])\n",
    "    return pd.Series(a), pd.Series(b)\n",
    "\n",
    "eng, deu = select(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:16.237975Z",
     "iopub.status.busy": "2024-10-29T02:32:16.237601Z",
     "iopub.status.idle": "2024-10-29T02:32:18.098162Z",
     "shell.execute_reply": "2024-10-29T02:32:18.097362Z",
     "shell.execute_reply.started": "2024-10-29T02:32:16.237940Z"
    },
    "papermill": {
     "duration": 1.934185,
     "end_time": "2024-10-28T08:42:19.711810",
     "exception": false,
     "start_time": "2024-10-28T08:42:17.777625",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng, deu = preprocess(eng), preprocess(deu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012828,
     "end_time": "2024-10-28T08:42:19.737778",
     "exception": false,
     "start_time": "2024-10-28T08:42:19.724950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### DATA\n",
    "https://www.kaggle.com/datasets/alincijov/bilingual-sentence-pairs/data\n",
    "Consist of translations of common sentences used in daily life, here we are using English and Deutch (German) for the Seq2Seq Machine translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:18.099896Z",
     "iopub.status.busy": "2024-10-29T02:32:18.099586Z",
     "iopub.status.idle": "2024-10-29T02:32:18.141656Z",
     "shell.execute_reply": "2024-10-29T02:32:18.140531Z",
     "shell.execute_reply.started": "2024-10-29T02:32:18.099864Z"
    },
    "papermill": {
     "duration": 0.055077,
     "end_time": "2024-10-28T08:42:19.805772",
     "exception": false,
     "start_time": "2024-10-28T08:42:19.750695",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'english' : eng, 'deutsch': deu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T16:37:51.800437Z",
     "iopub.status.busy": "2024-10-28T16:37:51.800108Z",
     "iopub.status.idle": "2024-10-28T16:37:51.825024Z",
     "shell.execute_reply": "2024-10-28T16:37:51.824050Z",
     "shell.execute_reply.started": "2024-10-28T16:37:51.800403Z"
    },
    "papermill": {
     "duration": 0.033329,
     "end_time": "2024-10-28T08:42:19.851894",
     "exception": false,
     "start_time": "2024-10-28T08:42:19.818565",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>deutsch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>geh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi</td>\n",
       "      <td>hallo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi</td>\n",
       "      <td>grüß gott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>lauf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run</td>\n",
       "      <td>lauf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wow</td>\n",
       "      <td>potzdonner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wow</td>\n",
       "      <td>donnerwetter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fire</td>\n",
       "      <td>feuer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>help</td>\n",
       "      <td>hilfe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>help</td>\n",
       "      <td>zu hülf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english       deutsch\n",
       "0      go           geh\n",
       "1      hi         hallo\n",
       "2      hi     grüß gott\n",
       "3     run          lauf\n",
       "4     run          lauf\n",
       "5     wow    potzdonner\n",
       "6     wow  donnerwetter\n",
       "7    fire         feuer\n",
       "8    help         hilfe\n",
       "9    help       zu hülf"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012599,
     "end_time": "2024-10-28T08:42:19.877934",
     "exception": false,
     "start_time": "2024-10-28T08:42:19.865335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr size=5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012638,
     "end_time": "2024-10-28T08:42:19.903371",
     "exception": false,
     "start_time": "2024-10-28T08:42:19.890733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Out of approximately 220k datapoints, I am considering only the first 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:24.314183Z",
     "iopub.status.busy": "2024-10-29T02:32:24.313471Z",
     "iopub.status.idle": "2024-10-29T02:32:24.318451Z",
     "shell.execute_reply": "2024-10-29T02:32:24.317416Z",
     "shell.execute_reply.started": "2024-10-29T02:32:24.314139Z"
    },
    "papermill": {
     "duration": 0.020788,
     "end_time": "2024-10-28T08:42:19.937461",
     "exception": false,
     "start_time": "2024-10-28T08:42:19.916673",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = data[:100_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013159,
     "end_time": "2024-10-28T08:42:19.963679",
     "exception": false,
     "start_time": "2024-10-28T08:42:19.950520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data preprocessing\n",
    "\n",
    "The data processing pipeline looks like :\n",
    "\n",
    "+ **Tokenization**: Breaking down text into smaller units (tokens), such as words or characters.\n",
    "+ **Numerical Encoding**: Assigning numerical representations to each token.\n",
    "+ **Sequence Padding**: Ensuring all sequences have the same length by adding padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:25.122812Z",
     "iopub.status.busy": "2024-10-29T02:32:25.122451Z",
     "iopub.status.idle": "2024-10-29T02:32:27.710020Z",
     "shell.execute_reply": "2024-10-29T02:32:27.708972Z",
     "shell.execute_reply.started": "2024-10-29T02:32:25.122781Z"
    },
    "papermill": {
     "duration": 2.55572,
     "end_time": "2024-10-28T08:42:22.532587",
     "exception": false,
     "start_time": "2024-10-28T08:42:19.976867",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "e_tokenizer,d_tokenizer = Tokenizer(), Tokenizer()\n",
    "e_tokenizer.fit_on_texts(data['english'])\n",
    "d_tokenizer.fit_on_texts(data['deutsch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:30.798231Z",
     "iopub.status.busy": "2024-10-29T02:32:30.797347Z",
     "iopub.status.idle": "2024-10-29T02:32:30.802678Z",
     "shell.execute_reply": "2024-10-29T02:32:30.801646Z",
     "shell.execute_reply.started": "2024-10-29T02:32:30.798189Z"
    },
    "papermill": {
     "duration": 0.020994,
     "end_time": "2024-10-28T08:42:22.567027",
     "exception": false,
     "start_time": "2024-10-28T08:42:22.546033",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "e_vocab, d_vocab  = len(e_tokenizer.word_index) + 1, len(d_tokenizer.word_index ) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012596,
     "end_time": "2024-10-28T08:42:22.592403",
     "exception": false,
     "start_time": "2024-10-28T08:42:22.579807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Maximum length of sentences\n",
    "To find the maximum number of words present in the dataset for both language. This isn't necessary and can be chosen subjectively as per required. However for this case I was experimenting a lot thus chose to let the dataset decide the value for this once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:31.472566Z",
     "iopub.status.busy": "2024-10-29T02:32:31.472170Z",
     "iopub.status.idle": "2024-10-29T02:32:31.589473Z",
     "shell.execute_reply": "2024-10-29T02:32:31.588692Z",
     "shell.execute_reply.started": "2024-10-29T02:32:31.472528Z"
    },
    "papermill": {
     "duration": 0.123804,
     "end_time": "2024-10-28T08:42:22.729018",
     "exception": false,
     "start_time": "2024-10-28T08:42:22.605214",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Len = lambda arr: max(len(i.split(\" \")) for i in arr)\n",
    "e_max_len, d_max_len = Len(data['english']), Len(data['deutsch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012782,
     "end_time": "2024-10-28T08:42:22.754771",
     "exception": false,
     "start_time": "2024-10-28T08:42:22.741989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Converting text to Numeric sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:33.862988Z",
     "iopub.status.busy": "2024-10-29T02:32:33.862562Z",
     "iopub.status.idle": "2024-10-29T02:32:36.998012Z",
     "shell.execute_reply": "2024-10-29T02:32:36.996982Z",
     "shell.execute_reply.started": "2024-10-29T02:32:33.862948Z"
    },
    "papermill": {
     "duration": 3.109414,
     "end_time": "2024-10-28T08:42:25.877064",
     "exception": false,
     "start_time": "2024-10-28T08:42:22.767650",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, max_len):\n",
    "    text =  tokenizer.texts_to_sequences(text)\n",
    "    text = pad_sequences(text, max_len, padding = 'post')\n",
    "    return text\n",
    "\n",
    "X, y = encode(data['english'],e_tokenizer,e_max_len), encode(data['deutsch'],d_tokenizer, d_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:37.000428Z",
     "iopub.status.busy": "2024-10-29T02:32:36.999984Z",
     "iopub.status.idle": "2024-10-29T02:32:37.026272Z",
     "shell.execute_reply": "2024-10-29T02:32:37.025411Z",
     "shell.execute_reply.started": "2024-10-29T02:32:37.000380Z"
    },
    "papermill": {
     "duration": 0.041396,
     "end_time": "2024-10-28T08:42:25.931746",
     "exception": false,
     "start_time": "2024-10-28T08:42:25.890350",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013152,
     "end_time": "2024-10-28T08:42:25.957950",
     "exception": false,
     "start_time": "2024-10-28T08:42:25.944798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr size=3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012668,
     "end_time": "2024-10-28T08:42:25.983485",
     "exception": false,
     "start_time": "2024-10-28T08:42:25.970817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"text-align:center;\">Base Seq2Seq Model</h1>\n",
    "\n",
    "Seq2Seq models are constructed using key components like Long Short-Term Memory (LSTM) units, which are specialized types of recurrent neural networks (RNNs) that effectively capture temporal dependencies in data. LSTMs help in processing sequences by remembering important information over longer periods and forgetting irrelevant details, making them useful for handling tasks involving sequences of varying lengths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012928,
     "end_time": "2024-10-28T08:42:26.009239",
     "exception": false,
     "start_time": "2024-10-28T08:42:25.996311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p style=\"text-align:center;\"> <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Ismhi-muID5ooWf3ZIQFFg.png\" alt=\"Seq2Seq\"> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013011,
     "end_time": "2024-10-28T08:42:26.035096",
     "exception": false,
     "start_time": "2024-10-28T08:42:26.022085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I found the following notebook https://www.kaggle.com/code/harshjain123/machine-translation-seq2seq-lstms by Harsh Jain very insightful to understand  this process, its will be a great place to understand the data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:01.439916Z",
     "iopub.status.busy": "2024-10-29T02:32:01.438914Z",
     "iopub.status.idle": "2024-10-29T02:32:01.450267Z",
     "shell.execute_reply": "2024-10-29T02:32:01.449370Z",
     "shell.execute_reply.started": "2024-10-29T02:32:01.439874Z"
    },
    "papermill": {
     "duration": 0.027012,
     "end_time": "2024-10-28T08:42:26.075103",
     "exception": false,
     "start_time": "2024-10-28T08:42:26.048091",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable(package=\"Custom\", name=\"S2S\")\n",
    "class S2S(Model):\n",
    "    def __init__(self, in_vocab, out_vocab, in_timesteps, out_timesteps, units, **kwargs):\n",
    "        super(S2S, self).__init__(**kwargs)\n",
    "        \n",
    "        self.in_vocab = in_vocab\n",
    "        self.out_vocab = out_vocab\n",
    "        self.in_timesteps = in_timesteps\n",
    "        self.out_timesteps = out_timesteps\n",
    "        self.units = units\n",
    "        \n",
    "        # Define the layers\n",
    "        self.embed = Embedding(input_dim=in_vocab, output_dim=units, mask_zero=True)\n",
    "        self.encoder_lstm = LSTM(units)\n",
    "        self.r_vector = RepeatVector(out_timesteps)\n",
    "        self.decoder_lstm = LSTM(units, return_sequences=True)\n",
    "        self.dense = Dense(out_vocab, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Define the forward pass\n",
    "        x = self.embed(inputs)                           # (batch size, in_timesteps, units)\n",
    "        x = self.encoder_lstm(x)                         # (batch size, units)\n",
    "        x = self.r_vector(x)                             # (batch size, out_timesteps, units)\n",
    "        x = self.decoder_lstm(x)                         # (batch size, out_timesteps, units)\n",
    "        output = self.dense(x)                           # (batch size, out_timesteps, out_vocab)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(S2S, self).get_config()\n",
    "        config.update({\n",
    "              'in_vocab': self.in_vocab,\n",
    "              'out_vocab': self.out_vocab,\n",
    "              'in_timesteps': self.in_timesteps,\n",
    "              'out_timesteps': self.out_timesteps,\n",
    "              'units': self.units\n",
    "          })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(\n",
    "            in_vocab=config['in_vocab'],\n",
    "            out_vocab=config['out_vocab'],\n",
    "            in_timesteps=config['in_timesteps'],\n",
    "            out_timesteps=config['out_timesteps'],\n",
    "            units=config['units']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:32:41.337577Z",
     "iopub.status.busy": "2024-10-29T02:32:41.337148Z",
     "iopub.status.idle": "2024-10-29T02:32:42.002213Z",
     "shell.execute_reply": "2024-10-29T02:32:42.001430Z",
     "shell.execute_reply.started": "2024-10-29T02:32:41.337538Z"
    },
    "papermill": {
     "duration": 0.888003,
     "end_time": "2024-10-28T08:42:26.976382",
     "exception": false,
     "start_time": "2024-10-28T08:42:26.088379",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seq2seq = S2S(e_vocab, d_vocab, e_max_len,d_max_len, 512)\n",
    "seq2seq.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "out_shape = (y_train.shape[0], y_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T08:42:27.003879Z",
     "iopub.status.busy": "2024-10-28T08:42:27.003525Z",
     "iopub.status.idle": "2024-10-28T09:13:22.690952Z",
     "shell.execute_reply": "2024-10-28T09:13:22.690007Z"
    },
    "papermill": {
     "duration": 1855.703177,
     "end_time": "2024-10-28T09:13:22.692899",
     "exception": false,
     "start_time": "2024-10-28T08:42:26.989722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "4000/4000 [==============================] - 68s 14ms/step - loss: 1.6727\n",
      "Epoch 2/15\n",
      "4000/4000 [==============================] - 55s 14ms/step - loss: 1.0835\n",
      "Epoch 3/15\n",
      "4000/4000 [==============================] - 56s 14ms/step - loss: 0.7700\n",
      "Epoch 4/15\n",
      "4000/4000 [==============================] - 55s 14ms/step - loss: 0.5697\n",
      "Epoch 5/15\n",
      "4000/4000 [==============================] - 55s 14ms/step - loss: 0.4473\n",
      "Epoch 6/15\n",
      "4000/4000 [==============================] - 55s 14ms/step - loss: 0.3701\n",
      "Epoch 7/15\n",
      "4000/4000 [==============================] - 56s 14ms/step - loss: 0.3175\n",
      "Epoch 8/15\n",
      "4000/4000 [==============================] - 57s 14ms/step - loss: 0.2805\n",
      "Epoch 9/15\n",
      "4000/4000 [==============================] - 54s 14ms/step - loss: 0.2514\n",
      "Epoch 10/15\n",
      "4000/4000 [==============================] - 53s 13ms/step - loss: 0.2299\n",
      "Epoch 11/15\n",
      "4000/4000 [==============================] - 53s 13ms/step - loss: 0.2135\n",
      "Epoch 12/15\n",
      "4000/4000 [==============================] - 55s 14ms/step - loss: 0.2004\n",
      "Epoch 13/15\n",
      "4000/4000 [==============================] - 54s 14ms/step - loss: 0.1908\n",
      "Epoch 14/15\n",
      "4000/4000 [==============================] - 54s 14ms/step - loss: 0.1831\n",
      "Epoch 15/15\n",
      "4000/4000 [==============================] - 53s 13ms/step - loss: 0.1770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113a143fc70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.fit(x=X_train, y = y_train.reshape(out_shape), epochs = 15, batch_size = 20, validation_batch_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e14b15ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"s2s\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  4724736   \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  2099200   \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               multiple                  2099200   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  8540424   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,463,560\n",
      "Trainable params: 17,463,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T09:13:27.702507Z",
     "iopub.status.busy": "2024-10-28T09:13:27.702066Z",
     "iopub.status.idle": "2024-10-28T09:28:55.920565Z",
     "shell.execute_reply": "2024-10-28T09:28:55.919722Z"
    },
    "papermill": {
     "duration": 930.753455,
     "end_time": "2024-10-28T09:28:55.922517",
     "exception": false,
     "start_time": "2024-10-28T09:13:25.169062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 103s 13ms/step - loss: 0.2218\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 100s 13ms/step - loss: 0.2212\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 100s 13ms/step - loss: 0.2205\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 100s 12ms/step - loss: 0.2179\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 100s 12ms/step - loss: 0.2169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113b792fcd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.fit(x=X_train, y = y_train.reshape(out_shape), epochs = 5, batch_size = 10, validation_batch_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.742786,
     "end_time": "2024-10-28T09:29:03.409242",
     "exception": false,
     "start_time": "2024-10-28T09:28:59.666456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(for 25 epochs)\n",
    "### Final Loss : 0.1930\n",
    "### Training Time : 45 minutes 55 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T09:29:10.944915Z",
     "iopub.status.busy": "2024-10-28T09:29:10.944183Z",
     "iopub.status.idle": "2024-10-28T09:29:11.643815Z",
     "shell.execute_reply": "2024-10-28T09:29:11.642782Z"
    },
    "papermill": {
     "duration": 4.454453,
     "end_time": "2024-10-28T09:29:11.646218",
     "exception": false,
     "start_time": "2024-10-28T09:29:07.191765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s_model_tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s_model_tf\\assets\n"
     ]
    }
   ],
   "source": [
    "seq2seq.save(\"s2s_model_tf\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.691926,
     "end_time": "2024-10-28T09:29:19.015899",
     "exception": false,
     "start_time": "2024-10-28T09:29:15.323973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Seq2Seq with Attention:\n",
    "\n",
    "Traditional Seq2Seq models often struggle with long input sequences. The encoder encodes the entire input sequence into a fixed-size vector, which can lead to information loss, especially for longer sequences.\n",
    "\n",
    "The attention mechanism addresses this limitation by allowing the decoder to focus on relevant parts of the input sequence at each decoding step. This enables the model to better capture long-range dependencies and produce more accurate outputs.\n",
    "\n",
    "\n",
    "Implemented using an attention layer that computes a context vector by attending to all encoder outputs.\n",
    "The context vector is combined with the decoder's hidden state at each timestep.\n",
    "\n",
    "\n",
    "+ Encoder LSTM now returns the full sequence of hidden states (encoder_outputs) and the final hidden state (state_h, state_c).\n",
    "+ Attention is applied at each timestep of the decoder to produce a context vector that is combined with the decoder input at that timestep.\n",
    "+ The attention mechanism allows the decoder to focus on different parts of the input sequence when producing each output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.676988,
     "end_time": "2024-10-28T09:29:26.490103",
     "exception": false,
     "start_time": "2024-10-28T09:29:22.813115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"text-align:center;\"> <img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/attention/general_scheme-min.png\n",
    "\" alt=\"Seq2Seq\"> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.684598,
     "end_time": "2024-10-28T09:29:33.870944",
     "exception": false,
     "start_time": "2024-10-28T09:29:30.186346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " Img sources : https://lena-voita.github.io/resources/lectures/seq2seq/attention/general_scheme-min.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.676251,
     "end_time": "2024-10-28T09:29:41.200097",
     "exception": false,
     "start_time": "2024-10-28T09:29:37.523846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Bahdanau Attention Layer:\n",
    "\n",
    "The model calculates alignment scores between the encoder's hidden states and the current decoder's hidden state at each decoding step. The relevance or significance of each encoder hidden state in relation to the current decoding phase is represented by these scores.\n",
    "\n",
    "+ Takes the encoder outputs (sequence of hidden states) and the decoder hidden state to compute the attention weights.\n",
    "+ Computes the context vector, which is a weighted sum of the encoder outputs based on attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:33:48.642806Z",
     "iopub.status.busy": "2024-10-29T02:33:48.642450Z",
     "iopub.status.idle": "2024-10-29T02:33:48.651508Z",
     "shell.execute_reply": "2024-10-29T02:33:48.650509Z",
     "shell.execute_reply.started": "2024-10-29T02:33:48.642775Z"
    },
    "papermill": {
     "duration": 3.839487,
     "end_time": "2024-10-28T09:29:48.711534",
     "exception": false,
     "start_time": "2024-10-28T09:29:44.872047",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        \n",
    "        super(Attention, self).__init__()\n",
    "        self.units = units\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "        \n",
    "        \n",
    "    def call(self, query, value):\n",
    "        \n",
    "        # query shape == (batch_size, hidden_size) -> decoder hidden state at the current timestep\n",
    "        # values shape == (batch_size, max_len, hidden_size) -> encoder outputs (all timesteps)\n",
    "        \n",
    "        q_time = tf.expand_dims(query, axis = 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(q_time)+self.W2(value)))\n",
    "        weights = tf.nn.softmax(score, axis=1)\n",
    "        context = weights * value\n",
    "        context = tf.reduce_sum(context, axis=1)          # (batch_size, hidden_size)\n",
    "        return context, weights\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Attention, self).get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.954086,
     "end_time": "2024-10-28T09:29:56.659820",
     "exception": false,
     "start_time": "2024-10-28T09:29:52.705734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Call action:\n",
    "\n",
    "+ After encoding, the attention mechanism is applied for each timestep in the decoder.\n",
    "+ The **context vector** and the **decoder input** at each timestep are concatenated and passed to the decoder LSTM.\n",
    "+ The result is a sequence of outputs, one for each decoder timestep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:33:49.720087Z",
     "iopub.status.busy": "2024-10-29T02:33:49.719699Z",
     "iopub.status.idle": "2024-10-29T02:33:49.736208Z",
     "shell.execute_reply": "2024-10-29T02:33:49.735185Z",
     "shell.execute_reply.started": "2024-10-29T02:33:49.720052Z"
    },
    "papermill": {
     "duration": 3.741481,
     "end_time": "2024-10-28T09:30:04.346303",
     "exception": false,
     "start_time": "2024-10-28T09:30:00.604822",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable(package=\"Custom\", name=\"S2SA\")\n",
    "class S2SA(Model):\n",
    "\n",
    "    def __init__(self, in_vocab, out_vocab, in_len, out_len, units, **kwargs):\n",
    "        super(S2SA, self).__init__(**kwargs)\n",
    "        \n",
    "        self.in_vocab = in_vocab\n",
    "        self.out_vocab = out_vocab\n",
    "        self.in_len = in_len\n",
    "        self.out_len = out_len\n",
    "        self.units = units\n",
    "        \n",
    "        self.embed = Embedding(input_dim=in_vocab, output_dim=units, mask_zero = True)\n",
    "        self.encoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.attention = Attention(units)\n",
    "        self.r_vectors = RepeatVector(out_len)\n",
    "        self.decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.dense = Dense(out_vocab, activation = 'softmax')\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embed(inputs)\n",
    "        e_out, e_h_state, e_c_state = self.encoder_lstm(x)       # (batch_size, in_timesteps, units), states\n",
    "        d_in = self.r_vectors(e_h_state)                       # (batch_size, out_timesteps, units)\n",
    "        \n",
    "        d_h_state, d_c_state = e_h_state, e_c_state\n",
    "        all_dec_out = []\n",
    "        \n",
    "\n",
    "        for t in range(d_in.shape[1]):\n",
    "            \n",
    "            d_at_t = d_in[:, t:t+1, :]                                            # (batch_size, 1, units) at t timestep\n",
    "            \n",
    "            context_vector,_= self.attention(e_h_state, e_out)                      # (batch_size, units)\n",
    "                                                                                  # TO MATCH THE DIMS OF D_in and context\n",
    "            context_vector = tf.expand_dims(context_vector, axis=1)               # (batch_size, 1, units)\n",
    "            \n",
    "            context_w_inputs = tf.concat([context_vector, d_at_t], axis = -1)     # (batch_size, 1, 2 x units)\n",
    "            \n",
    "            d_out,d_h_state, d_c_state = self.decoder_lstm(context_w_inputs, initial_state = [d_h_state, d_c_state])\n",
    "            d_out = self.dense(d_out)\n",
    "            \n",
    "            all_dec_out.append(d_out)\n",
    "            \n",
    "        \n",
    "        d_out = tf.concat(all_dec_out, axis = 1)            # To aggregate outputs across timesteps\n",
    "        \n",
    "        return d_out\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(S2SA, self).get_config()\n",
    "        config.update({\n",
    "              'in_vocab': self.in_vocab,\n",
    "              'out_vocab': self.out_vocab,\n",
    "              'in_len': self.in_len,\n",
    "              'out_len': self.out_len,\n",
    "              'units': self.units\n",
    "          })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(\n",
    "            in_vocab=config['in_vocab'],\n",
    "            out_vocab=config['out_vocab'],\n",
    "            in_len=config['in_len'],\n",
    "            out_len=config['out_len'],\n",
    "            units=config['units']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:33:51.938609Z",
     "iopub.status.busy": "2024-10-29T02:33:51.937987Z",
     "iopub.status.idle": "2024-10-29T02:33:51.963802Z",
     "shell.execute_reply": "2024-10-29T02:33:51.962834Z",
     "shell.execute_reply.started": "2024-10-29T02:33:51.938570Z"
    },
    "papermill": {
     "duration": 3.93287,
     "end_time": "2024-10-28T09:30:12.227086",
     "exception": false,
     "start_time": "2024-10-28T09:30:08.294216",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attention_model = S2SA(e_vocab, d_vocab, e_max_len, d_max_len, 512)\n",
    "attention_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T09:30:20.054005Z",
     "iopub.status.busy": "2024-10-28T09:30:20.053559Z",
     "iopub.status.idle": "2024-10-28T13:50:18.802035Z",
     "shell.execute_reply": "2024-10-28T13:50:18.800661Z"
    },
    "papermill": {
     "duration": 15602.676988,
     "end_time": "2024-10-28T13:50:18.804239",
     "exception": false,
     "start_time": "2024-10-28T09:30:16.127251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "4000/4000 [==============================] - 229s 52ms/step - loss: 1.4942\n",
      "Epoch 2/15\n",
      "4000/4000 [==============================] - 210s 53ms/step - loss: 0.9004\n",
      "Epoch 3/15\n",
      "4000/4000 [==============================] - 210s 53ms/step - loss: 0.6491\n",
      "Epoch 4/15\n",
      "4000/4000 [==============================] - 213s 53ms/step - loss: 0.5055\n",
      "Epoch 5/15\n",
      "4000/4000 [==============================] - 218s 55ms/step - loss: 0.4163\n",
      "Epoch 6/15\n",
      "4000/4000 [==============================] - 218s 55ms/step - loss: 0.3571\n",
      "Epoch 7/15\n",
      "4000/4000 [==============================] - 215s 54ms/step - loss: 0.3149\n",
      "Epoch 8/15\n",
      "4000/4000 [==============================] - 214s 53ms/step - loss: 0.2824\n",
      "Epoch 9/15\n",
      "4000/4000 [==============================] - 217s 54ms/step - loss: 0.2561\n",
      "Epoch 10/15\n",
      "4000/4000 [==============================] - 215s 54ms/step - loss: 0.2362\n",
      "Epoch 11/15\n",
      "4000/4000 [==============================] - 213s 53ms/step - loss: 0.2194\n",
      "Epoch 12/15\n",
      "4000/4000 [==============================] - 209s 52ms/step - loss: 0.2063\n",
      "Epoch 13/15\n",
      "4000/4000 [==============================] - 215s 54ms/step - loss: 0.1958\n",
      "Epoch 14/15\n",
      "4000/4000 [==============================] - 213s 53ms/step - loss: 0.1870\n",
      "Epoch 15/15\n",
      "4000/4000 [==============================] - 213s 53ms/step - loss: 0.1801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113b844e700>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_model.fit(X_train, y_train.reshape(out_shape), epochs = 15, batch_size=20, validation_batch_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53b7108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"s2sa\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  4724736   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               multiple                  2099200   \n",
      "                                                                 \n",
      " attention (Attention)       multiple                  525825    \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVect  multiple                 0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               multiple                  3147776   \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  8540424   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,037,961\n",
      "Trainable params: 19,037,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attention_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T16:38:33.658215Z",
     "iopub.status.busy": "2024-10-28T16:38:33.657745Z",
     "iopub.status.idle": "2024-10-28T17:32:11.541783Z",
     "shell.execute_reply": "2024-10-28T17:32:11.540709Z",
     "shell.execute_reply.started": "2024-10-28T16:38:33.658172Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 484s 59ms/step - loss: 0.2156\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 469s 59ms/step - loss: 0.2122\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 480s 60ms/step - loss: 0.2089\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 469s 59ms/step - loss: 0.2059\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 472s 59ms/step - loss: 0.2021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11767a5cf40>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_model.fit(X_train, y_train.reshape(out_shape), epochs = 5, batch_size=10, validation_batch_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 12.459312,
     "end_time": "2024-10-28T15:31:57.138653",
     "exception": false,
     "start_time": "2024-10-28T15:31:44.679341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(for 25 epochs)\n",
    "### Final Loss : 0.1697\n",
    "### Training Time : 5 hours, 45 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:32:52.218373Z",
     "iopub.status.busy": "2024-10-28T17:32:52.217945Z",
     "iopub.status.idle": "2024-10-28T17:32:53.016456Z",
     "shell.execute_reply": "2024-10-28T17:32:53.015632Z",
     "shell.execute_reply.started": "2024-10-28T17:32:52.218331Z"
    },
    "papermill": {
     "duration": 13.079886,
     "end_time": "2024-10-28T15:32:22.622942",
     "exception": false,
     "start_time": "2024-10-28T15:32:09.543056",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2sa\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2sa\\assets\n",
      "WARNING:absl:<__main__.Attention object at 0x00000116D8CBE310> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# attention_model.save(\"s2sa.keras\")\n",
    "attention_model.save(\"s2sa\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 12.320792,
     "end_time": "2024-10-28T15:32:47.348358",
     "exception": false,
     "start_time": "2024-10-28T15:32:35.027566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Translation comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:33:30.412065Z",
     "iopub.status.busy": "2024-10-28T17:33:30.411336Z",
     "iopub.status.idle": "2024-10-28T17:33:30.419068Z",
     "shell.execute_reply": "2024-10-28T17:33:30.418107Z",
     "shell.execute_reply.started": "2024-10-28T17:33:30.412024Z"
    },
    "papermill": {
     "duration": 12.387401,
     "end_time": "2024-10-28T15:33:12.231348",
     "exception": false,
     "start_time": "2024-10-28T15:32:59.843947",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test(model1, model2, batch_size, size=len(X_test),):\n",
    "    p = np.argmax(model1.predict(X_test[:size], batch_size=batch_size), axis=-1)\n",
    "    q = np.argmax(model2.predict(X_test[:size], batch_size=batch_size), axis=-1)\n",
    "    real = e_tokenizer.sequences_to_texts(X_test[:size])\n",
    "    pred1 = d_tokenizer.sequences_to_texts(p)\n",
    "    pred2 = d_tokenizer.sequences_to_texts(q)\n",
    "    trans = d_tokenizer.sequences_to_texts(y_test[:size])\n",
    "    d = {'Eng': real,'Deu': trans, 'Base_Model': pred1, 'Attention_Model': pred2}\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:33:24.977937Z",
     "iopub.status.busy": "2024-10-29T02:33:24.977225Z",
     "iopub.status.idle": "2024-10-29T02:33:28.873330Z",
     "shell.execute_reply": "2024-10-29T02:33:28.872153Z",
     "shell.execute_reply.started": "2024-10-29T02:33:24.977896Z"
    },
    "papermill": {
     "duration": 14.465988,
     "end_time": "2024-10-28T15:33:38.984418",
     "exception": false,
     "start_time": "2024-10-28T15:33:24.518430",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "s = {'S2S':S2S}\n",
    "seq2seq = keras.models.load_model('s2s_model_tf', custom_objects=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T02:34:11.163101Z",
     "iopub.status.busy": "2024-10-29T02:34:11.162729Z",
     "iopub.status.idle": "2024-10-29T02:34:16.783046Z",
     "shell.execute_reply": "2024-10-29T02:34:16.782063Z",
     "shell.execute_reply.started": "2024-10-29T02:34:11.163065Z"
    },
    "papermill": {
     "duration": 16.498854,
     "end_time": "2024-10-28T15:34:07.968783",
     "exception": false,
     "start_time": "2024-10-28T15:33:51.469929",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sa = {'S2SA':S2SA}\n",
    "attention_model = keras.models.load_model('s2sa', custom_objects=sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:33:14.646952Z",
     "iopub.status.busy": "2024-10-28T17:33:14.646565Z",
     "iopub.status.idle": "2024-10-28T17:33:14.930205Z",
     "shell.execute_reply": "2024-10-28T17:33:14.929168Z",
     "shell.execute_reply.started": "2024-10-28T17:33:14.646915Z"
    },
    "papermill": {
     "duration": 12.666187,
     "end_time": "2024-10-28T15:34:33.013958",
     "exception": false,
     "start_time": "2024-10-28T15:34:20.347771",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:33:33.337300Z",
     "iopub.status.busy": "2024-10-28T17:33:33.336168Z",
     "iopub.status.idle": "2024-10-28T17:33:56.976907Z",
     "shell.execute_reply": "2024-10-28T17:33:56.975907Z",
     "shell.execute_reply.started": "2024-10-28T17:33:33.337216Z"
    },
    "papermill": {
     "duration": 43.479333,
     "end_time": "2024-10-28T15:35:28.801320",
     "exception": false,
     "start_time": "2024-10-28T15:34:45.321987",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 4s 4ms/step\n",
      "400/400 [==============================] - 49s 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eng</th>\n",
       "      <th>Deu</th>\n",
       "      <th>Base_Model</th>\n",
       "      <th>Attention_Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>she decided to marry tom</td>\n",
       "      <td>sie hat sich entschieden tom zu heiraten</td>\n",
       "      <td>sie entschied sich tom zu heiraten heiraten</td>\n",
       "      <td>sie entschied sich tom zu heiraten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do not read while walking</td>\n",
       "      <td>lies nicht im gehen</td>\n",
       "      <td>lies sie am am lesen</td>\n",
       "      <td>schlaf nicht nicht mehr mehr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which ones mine</td>\n",
       "      <td>welche ist meine</td>\n",
       "      <td>welcher ist meiner</td>\n",
       "      <td>welcher ist meiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this knife is very sharp</td>\n",
       "      <td>dieses messer ist sehr scharf</td>\n",
       "      <td>dieses bier ist sehr trocken</td>\n",
       "      <td>dieser messer ist sehr scharf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that was just plain stupid</td>\n",
       "      <td>das war einfach nur dumm</td>\n",
       "      <td>das war schlicht und ergreifend dumm</td>\n",
       "      <td>das war schlicht und ergreifend dumm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>theres still a lot left</td>\n",
       "      <td>es gibt noch immer eine menge zu tun</td>\n",
       "      <td>es ist noch viel übrig</td>\n",
       "      <td>es ist noch noch übrig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>please add up the numbers</td>\n",
       "      <td>bitte addiert die zahlen</td>\n",
       "      <td>bitte addiere die zahlen</td>\n",
       "      <td>bitte addiere die die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>he is playing in his room</td>\n",
       "      <td>er spielt in seinem zimmer</td>\n",
       "      <td>er wohnt in einem zimmer</td>\n",
       "      <td>er spielt in in zimmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tom isnt your brother</td>\n",
       "      <td>tom ist nicht dein bruder</td>\n",
       "      <td>tom ist nicht dein bruder</td>\n",
       "      <td>tom ist nicht dein bruder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>im not going to stop</td>\n",
       "      <td>ich werde nicht aufhören</td>\n",
       "      <td>ich werde nicht aufhören</td>\n",
       "      <td>ich werde nicht anhalten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i cant stand kids</td>\n",
       "      <td>ich hasse kinder</td>\n",
       "      <td>ich kann nicht kinder nicht</td>\n",
       "      <td>ich kann keine kinder ausstehen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i want to study math</td>\n",
       "      <td>ich möchte mathematik studieren</td>\n",
       "      <td>ich will mathe studieren</td>\n",
       "      <td>ich möchte mathematik werden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tom is very arrogant</td>\n",
       "      <td>tom ist ziemlich arrogant</td>\n",
       "      <td>tom ist sehr arrogant</td>\n",
       "      <td>tom ist sehr arrogant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i rubbed my feet</td>\n",
       "      <td>ich rieb meine füße</td>\n",
       "      <td>ich meine meine meine</td>\n",
       "      <td>ich füße mir füße füße</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>can we fish there</td>\n",
       "      <td>kann man hier angeln</td>\n",
       "      <td>krieg wir jetzt fisch</td>\n",
       "      <td>können sie dort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>do you come here every day</td>\n",
       "      <td>kommst du hier jeden tag her</td>\n",
       "      <td>kommt du jeden tag</td>\n",
       "      <td>reisen du jeden tag hierher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>go away</td>\n",
       "      <td>verzieh dich</td>\n",
       "      <td>scher dich</td>\n",
       "      <td>verdufte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>they cheat</td>\n",
       "      <td>sie mogeln</td>\n",
       "      <td>sie schummeln</td>\n",
       "      <td>sie schummeln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>can you order it for me</td>\n",
       "      <td>können sie ihn mir bestellen</td>\n",
       "      <td>kannst du es mir</td>\n",
       "      <td>kannst dus mir mir bestellen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tom put on some clothes</td>\n",
       "      <td>tom zog sich an</td>\n",
       "      <td>tom hat sich angezogen</td>\n",
       "      <td>tom hat sich angezogen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Eng                                       Deu  \\\n",
       "0     she decided to marry tom  sie hat sich entschieden tom zu heiraten   \n",
       "1    do not read while walking                       lies nicht im gehen   \n",
       "2              which ones mine                          welche ist meine   \n",
       "3     this knife is very sharp             dieses messer ist sehr scharf   \n",
       "4   that was just plain stupid                  das war einfach nur dumm   \n",
       "5      theres still a lot left      es gibt noch immer eine menge zu tun   \n",
       "6    please add up the numbers                  bitte addiert die zahlen   \n",
       "7    he is playing in his room                er spielt in seinem zimmer   \n",
       "8        tom isnt your brother                 tom ist nicht dein bruder   \n",
       "9         im not going to stop                  ich werde nicht aufhören   \n",
       "10           i cant stand kids                          ich hasse kinder   \n",
       "11        i want to study math           ich möchte mathematik studieren   \n",
       "12        tom is very arrogant                 tom ist ziemlich arrogant   \n",
       "13            i rubbed my feet                       ich rieb meine füße   \n",
       "14           can we fish there                      kann man hier angeln   \n",
       "15  do you come here every day              kommst du hier jeden tag her   \n",
       "16                     go away                              verzieh dich   \n",
       "17                  they cheat                                sie mogeln   \n",
       "18     can you order it for me              können sie ihn mir bestellen   \n",
       "19     tom put on some clothes                           tom zog sich an   \n",
       "\n",
       "                                     Base_Model  \\\n",
       "0   sie entschied sich tom zu heiraten heiraten   \n",
       "1                          lies sie am am lesen   \n",
       "2                            welcher ist meiner   \n",
       "3                  dieses bier ist sehr trocken   \n",
       "4          das war schlicht und ergreifend dumm   \n",
       "5                        es ist noch viel übrig   \n",
       "6                      bitte addiere die zahlen   \n",
       "7                      er wohnt in einem zimmer   \n",
       "8                     tom ist nicht dein bruder   \n",
       "9                      ich werde nicht aufhören   \n",
       "10                  ich kann nicht kinder nicht   \n",
       "11                     ich will mathe studieren   \n",
       "12                        tom ist sehr arrogant   \n",
       "13                        ich meine meine meine   \n",
       "14                        krieg wir jetzt fisch   \n",
       "15                           kommt du jeden tag   \n",
       "16                                   scher dich   \n",
       "17                                sie schummeln   \n",
       "18                             kannst du es mir   \n",
       "19                       tom hat sich angezogen   \n",
       "\n",
       "                         Attention_Model  \n",
       "0     sie entschied sich tom zu heiraten  \n",
       "1           schlaf nicht nicht mehr mehr  \n",
       "2                     welcher ist meiner  \n",
       "3          dieser messer ist sehr scharf  \n",
       "4   das war schlicht und ergreifend dumm  \n",
       "5                 es ist noch noch übrig  \n",
       "6                  bitte addiere die die  \n",
       "7                 er spielt in in zimmer  \n",
       "8              tom ist nicht dein bruder  \n",
       "9               ich werde nicht anhalten  \n",
       "10       ich kann keine kinder ausstehen  \n",
       "11          ich möchte mathematik werden  \n",
       "12                 tom ist sehr arrogant  \n",
       "13                ich füße mir füße füße  \n",
       "14                       können sie dort  \n",
       "15           reisen du jeden tag hierher  \n",
       "16                              verdufte  \n",
       "17                         sie schummeln  \n",
       "18          kannst dus mir mir bestellen  \n",
       "19                tom hat sich angezogen  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = test(seq2seq,attention_model, 10,4000)\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:33:56.979590Z",
     "iopub.status.busy": "2024-10-28T17:33:56.979076Z",
     "iopub.status.idle": "2024-10-28T17:33:56.986802Z",
     "shell.execute_reply": "2024-10-28T17:33:56.985799Z",
     "shell.execute_reply.started": "2024-10-28T17:33:56.979543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test1(model1, model2, batch_size, size=len(X_test),):\n",
    "    p = np.argmax(model1.predict(X[:size], batch_size=batch_size), axis=-1)\n",
    "    q = np.argmax(model2.predict(X[:size], batch_size=batch_size), axis=-1)\n",
    "    real = e_tokenizer.sequences_to_texts(X[:size])\n",
    "    pred1 = d_tokenizer.sequences_to_texts(p)\n",
    "    pred2 = d_tokenizer.sequences_to_texts(q)\n",
    "    trans = d_tokenizer.sequences_to_texts(y[:size])\n",
    "    d = {'Eng': real,'Deu': trans, 'Base_Model': pred1, 'Attention_Model': pred2}\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:33:56.988119Z",
     "iopub.status.busy": "2024-10-28T17:33:56.987847Z",
     "iopub.status.idle": "2024-10-28T17:34:18.188535Z",
     "shell.execute_reply": "2024-10-28T17:34:18.187454Z",
     "shell.execute_reply.started": "2024-10-28T17:33:56.988089Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 5s 11ms/step\n",
      "400/400 [==============================] - 15s 37ms/step\n"
     ]
    }
   ],
   "source": [
    "df_test = test1(seq2seq,attention_model, 10,4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:36:35.119325Z",
     "iopub.status.busy": "2024-10-28T17:36:35.116548Z",
     "iopub.status.idle": "2024-10-28T17:36:35.137163Z",
     "shell.execute_reply": "2024-10-28T17:36:35.136276Z",
     "shell.execute_reply.started": "2024-10-28T17:36:35.119275Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eng</th>\n",
       "      <th>Deu</th>\n",
       "      <th>Base_Model</th>\n",
       "      <th>Attention_Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>wake up</td>\n",
       "      <td>wachen sie auf</td>\n",
       "      <td>wach auf</td>\n",
       "      <td>wach auf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>wake up</td>\n",
       "      <td>wach auf</td>\n",
       "      <td>wach auf</td>\n",
       "      <td>wach auf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>wake up</td>\n",
       "      <td>wachen sie auf</td>\n",
       "      <td>wach auf</td>\n",
       "      <td>wach auf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>wash up</td>\n",
       "      <td>wasch dir die hände</td>\n",
       "      <td>wasch dir das gesicht</td>\n",
       "      <td>wasch dir die getreten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>wash up</td>\n",
       "      <td>wasch dir das gesicht</td>\n",
       "      <td>wasch dir das gesicht</td>\n",
       "      <td>wasch dir die getreten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>we lost</td>\n",
       "      <td>wir haben verloren</td>\n",
       "      <td>wir haben verloren verirrt</td>\n",
       "      <td>wir haben verloren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>welcome</td>\n",
       "      <td>willkommen</td>\n",
       "      <td>willkommen</td>\n",
       "      <td>willkommen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>who ate</td>\n",
       "      <td>wer hat gegessen</td>\n",
       "      <td>wer aß gegessen</td>\n",
       "      <td>wer hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>who ate</td>\n",
       "      <td>wer aß</td>\n",
       "      <td>wer aß gegessen</td>\n",
       "      <td>wer hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>who ran</td>\n",
       "      <td>wer rannte</td>\n",
       "      <td>wer rannte</td>\n",
       "      <td>wer rannte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>who ran</td>\n",
       "      <td>wer ist gerannt</td>\n",
       "      <td>wer rannte</td>\n",
       "      <td>wer rannte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>who won</td>\n",
       "      <td>wer hat gewonnen</td>\n",
       "      <td>wer hat gewonnen gewonnen</td>\n",
       "      <td>wer hat gewonnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>you run</td>\n",
       "      <td>du läufst</td>\n",
       "      <td>du laufen</td>\n",
       "      <td>du laufen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>you run</td>\n",
       "      <td>sie laufen</td>\n",
       "      <td>du laufen</td>\n",
       "      <td>du laufen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>you won</td>\n",
       "      <td>du hast gewonnen</td>\n",
       "      <td>du hast gewonnen</td>\n",
       "      <td>du hast gewonnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>all rise</td>\n",
       "      <td>alle aufstehen</td>\n",
       "      <td>alle aufstehen</td>\n",
       "      <td>alle aufstehen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>am i fat</td>\n",
       "      <td>bin ich dick</td>\n",
       "      <td>bin ich dick</td>\n",
       "      <td>bin ich dick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ask them</td>\n",
       "      <td>frag sie</td>\n",
       "      <td>frag sie</td>\n",
       "      <td>frag sie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>back off</td>\n",
       "      <td>komm nicht näher</td>\n",
       "      <td>komm nicht weg</td>\n",
       "      <td>komm näher näher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>be a man</td>\n",
       "      <td>sei ein mann</td>\n",
       "      <td>sei ein mann</td>\n",
       "      <td>sei ein mann</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Eng                    Deu                  Base_Model  \\\n",
       "290   wake up         wachen sie auf                    wach auf   \n",
       "291   wake up               wach auf                    wach auf   \n",
       "292   wake up         wachen sie auf                    wach auf   \n",
       "293   wash up    wasch dir die hände       wasch dir das gesicht   \n",
       "294   wash up  wasch dir das gesicht       wasch dir das gesicht   \n",
       "295   we lost     wir haben verloren  wir haben verloren verirrt   \n",
       "296   welcome             willkommen                  willkommen   \n",
       "297   who ate       wer hat gegessen             wer aß gegessen   \n",
       "298   who ate                 wer aß             wer aß gegessen   \n",
       "299   who ran             wer rannte                  wer rannte   \n",
       "300   who ran        wer ist gerannt                  wer rannte   \n",
       "301   who won       wer hat gewonnen   wer hat gewonnen gewonnen   \n",
       "302   you run              du läufst                   du laufen   \n",
       "303   you run             sie laufen                   du laufen   \n",
       "304   you won       du hast gewonnen            du hast gewonnen   \n",
       "305  all rise         alle aufstehen              alle aufstehen   \n",
       "306  am i fat           bin ich dick                bin ich dick   \n",
       "307  ask them               frag sie                    frag sie   \n",
       "308  back off       komm nicht näher              komm nicht weg   \n",
       "309  be a man           sei ein mann                sei ein mann   \n",
       "\n",
       "            Attention_Model  \n",
       "290                wach auf  \n",
       "291                wach auf  \n",
       "292                wach auf  \n",
       "293  wasch dir die getreten  \n",
       "294  wasch dir die getreten  \n",
       "295      wir haben verloren  \n",
       "296              willkommen  \n",
       "297                 wer hat  \n",
       "298                 wer hat  \n",
       "299              wer rannte  \n",
       "300              wer rannte  \n",
       "301        wer hat gewonnen  \n",
       "302               du laufen  \n",
       "303               du laufen  \n",
       "304        du hast gewonnen  \n",
       "305          alle aufstehen  \n",
       "306            bin ich dick  \n",
       "307                frag sie  \n",
       "308        komm näher näher  \n",
       "309            sei ein mann  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[290:310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:37:27.427497Z",
     "iopub.status.busy": "2024-10-28T17:37:27.427078Z",
     "iopub.status.idle": "2024-10-28T17:37:27.431717Z",
     "shell.execute_reply": "2024-10-28T17:37:27.430835Z",
     "shell.execute_reply.started": "2024-10-28T17:37:27.427456Z"
    }
   },
   "source": [
    "## Amount of exact translations by each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:34:18.191795Z",
     "iopub.status.busy": "2024-10-28T17:34:18.190987Z",
     "iopub.status.idle": "2024-10-28T17:34:18.200320Z",
     "shell.execute_reply": "2024-10-28T17:34:18.199186Z",
     "shell.execute_reply.started": "2024-10-28T17:34:18.191738Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test['Deu'] == df_test['Base_Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:34:18.202005Z",
     "iopub.status.busy": "2024-10-28T17:34:18.201638Z",
     "iopub.status.idle": "2024-10-28T17:34:18.214497Z",
     "shell.execute_reply": "2024-10-28T17:34:18.213630Z",
     "shell.execute_reply.started": "2024-10-28T17:34:18.201963Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1972"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test['Deu'] == df_test['Attention_Model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 12.229396,
     "end_time": "2024-10-28T15:36:42.733808",
     "exception": false,
     "start_time": "2024-10-28T15:36:30.504412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Observations and Conclusion\n",
    "\n",
    "\n",
    "**Postivites :**\n",
    "+ Translations generated by model equipped with attention were more accurate than with the model without, \n",
    "+ The model without attention was less likely to mistake one or a few words in a sentence, hence better semantic awareness\n",
    "+ After more epochs the model with attention coverges at a much lower loss than the model without, however due to hardware limitations I couldn't show it in the same notebook.\n",
    "\n",
    "**Negetives :**\n",
    "+ Model with attention takes significanly much time to train approximately times longer (5+ hours as compared to 45 minutes)\n",
    "+ Model with attention takes *4-5 times longer to run* (approx. 21ms) compared to the model without (approx. 2ms)  When Inferened on GPU P100\n",
    "\n",
    "\n",
    "Attention mechanisms have become an essential component of modern sequence-to-sequence models, despite longer training time. By allowing the model to focus on relevant parts of the input sequence, attention helps to improve performance, interpretability, and flexibility.\n",
    "\n",
    "\n",
    "## But why pay 'Attention'?\n",
    "\n",
    "Since models RNN models which are used sequence modelling need attention despite being specifically designed for the same purpose, it becomes very apparent how powerful attention can be in sequential modelling\n",
    "\n",
    "\n",
    "Transformer models, which rely entirely on attention mechanisms, have revolutionized the field of natural language processing. They have achieved state-of-the-art results on various tasks, including machine translation, text summarization, and question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 12.295254,
     "end_time": "2024-10-28T15:37:07.313983",
     "exception": false,
     "start_time": "2024-10-28T15:36:55.018729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Referenes\n",
    "+ https://www.kaggle.com/code/harshjain123/machine-translation-seq2seq-lstms : A very helpful notebook for understanding the data processing pipeline\n",
    "+ https://www.kaggle.com/datasets/alincijov/bilingual-sentence-pairs : Bilingual pair dataset\n",
    "\n",
    "+ https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning : A good Introduction to seq2seq modelling\n",
    "+ https://youtu.be/yInilk6x-OY?si=2e6MOB_DdflA60Ar : A great Lecture on attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd74f1d6",
   "metadata": {},
   "source": [
    "# Experiements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018c5d3",
   "metadata": {},
   "source": [
    "## Seq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f55ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = S2S(e_vocab, d_vocab, e_max_len,d_max_len, 512)\n",
    "seq2seq.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "out_shape = (y_train.shape[0], y_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0301a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "1250/1250 [==============================] - 33s 24ms/step - loss: 1.5816\n",
      "Epoch 2/32\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 1.1983\n",
      "Epoch 3/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 1.0343\n",
      "Epoch 4/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.9362\n",
      "Epoch 5/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.8660\n",
      "Epoch 6/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.8132\n",
      "Epoch 7/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.7671\n",
      "Epoch 8/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.7317\n",
      "Epoch 9/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.6946\n",
      "Epoch 10/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.6606\n",
      "Epoch 11/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.6344\n",
      "Epoch 12/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.6085\n",
      "Epoch 13/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.5796\n",
      "Epoch 14/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.5558\n",
      "Epoch 15/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.5324\n",
      "Epoch 16/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.5105\n",
      "Epoch 17/32\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.4913\n",
      "Epoch 18/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.4736\n",
      "Epoch 19/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.4559\n",
      "Epoch 20/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.4400\n",
      "Epoch 21/32\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.4251\n",
      "Epoch 22/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.4111\n",
      "Epoch 23/32\n",
      "1250/1250 [==============================] - 29s 24ms/step - loss: 0.4003\n",
      "Epoch 24/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3889\n",
      "Epoch 25/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3785\n",
      "Epoch 26/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3697\n",
      "Epoch 27/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3615\n",
      "Epoch 28/32\n",
      "1250/1250 [==============================] - 29s 24ms/step - loss: 0.3542\n",
      "Epoch 29/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3476\n",
      "Epoch 30/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3428\n",
      "Epoch 31/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3378\n",
      "Epoch 32/32\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c90fa2dc0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.fit(x=X_train, y = y_train.reshape(out_shape), epochs = 32, batch_size = 64, validation_batch_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48d5483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"s2s_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  4724736   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               multiple                  2099200   \n",
      "                                                                 \n",
      " repeat_vector_2 (RepeatVect  multiple                 0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               multiple                  2099200   \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  8540424   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,463,560\n",
      "Trainable params: 17,463,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d4fecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2500/2500 [==============================] - 50s 19ms/step - loss: 0.3882\n",
      "Epoch 2/16\n",
      "2500/2500 [==============================] - 48s 19ms/step - loss: 0.4145\n",
      "Epoch 3/16\n",
      "2500/2500 [==============================] - 47s 19ms/step - loss: 0.4217\n",
      "Epoch 4/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.4235\n",
      "Epoch 5/16\n",
      "2500/2500 [==============================] - 46s 19ms/step - loss: 0.4217\n",
      "Epoch 6/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.4171\n",
      "Epoch 7/16\n",
      "2500/2500 [==============================] - 47s 19ms/step - loss: 0.4090\n",
      "Epoch 8/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.4003\n",
      "Epoch 9/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.3898\n",
      "Epoch 10/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.3803\n",
      "Epoch 11/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.3684\n",
      "Epoch 12/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.3585\n",
      "Epoch 13/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.3471\n",
      "Epoch 14/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.3374\n",
      "Epoch 15/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.3275\n",
      "Epoch 16/16\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.3189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22ca1ae2850>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.fit(x=X_train, y = y_train.reshape(out_shape), epochs = 16, batch_size = 32, validation_batch_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2353048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "5000/5000 [==============================] - 75s 15ms/step - loss: 0.3757\n",
      "Epoch 2/8\n",
      "5000/5000 [==============================] - 76s 15ms/step - loss: 0.4174\n",
      "Epoch 3/8\n",
      "5000/5000 [==============================] - 77s 15ms/step - loss: 0.4253\n",
      "Epoch 4/8\n",
      "5000/5000 [==============================] - 77s 15ms/step - loss: 0.4247\n",
      "Epoch 5/8\n",
      "5000/5000 [==============================] - 77s 15ms/step - loss: 0.4192\n",
      "Epoch 6/8\n",
      "5000/5000 [==============================] - 77s 15ms/step - loss: 0.4126\n",
      "Epoch 7/8\n",
      "5000/5000 [==============================] - 77s 15ms/step - loss: 0.4045\n",
      "Epoch 8/8\n",
      "5000/5000 [==============================] - 77s 15ms/step - loss: 0.3945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2294b053850>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.fit(x=X_train, y = y_train.reshape(out_shape), epochs = 8, batch_size = 16, validation_batch_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c48c12",
   "metadata": {},
   "source": [
    "(for 25 epochs)\n",
    "### Final Loss : 0.1930\n",
    "### Training Time : 45 minutes 55 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e02345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s_model_tf_v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s_model_tf_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "seq2seq.save(\"s2s_model_tf_v2\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba518ad",
   "metadata": {},
   "source": [
    "## Seq2Seq with Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08d94349",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = S2SA(e_vocab, d_vocab, e_max_len, d_max_len, 512)\n",
    "attention_model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7e000fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "1250/1250 [==============================] - 107s 70ms/step - loss: 1.4889\n",
      "Epoch 2/32\n",
      "1250/1250 [==============================] - 88s 70ms/step - loss: 1.1035\n",
      "Epoch 3/32\n",
      "1250/1250 [==============================] - 88s 70ms/step - loss: 0.9521\n",
      "Epoch 4/32\n",
      "1250/1250 [==============================] - 87s 70ms/step - loss: 0.8615\n",
      "Epoch 5/32\n",
      "1250/1250 [==============================] - 88s 70ms/step - loss: 0.7992\n",
      "Epoch 6/32\n",
      "1250/1250 [==============================] - 88s 70ms/step - loss: 0.7514\n",
      "Epoch 7/32\n",
      "1250/1250 [==============================] - 87s 70ms/step - loss: 0.7133\n",
      "Epoch 8/32\n",
      "1250/1250 [==============================] - 87s 69ms/step - loss: 0.6773\n",
      "Epoch 9/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.6446\n",
      "Epoch 10/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.6147\n",
      "Epoch 11/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.5900\n",
      "Epoch 12/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.5668\n",
      "Epoch 13/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.5423\n",
      "Epoch 14/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.5211\n",
      "Epoch 15/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.4999\n",
      "Epoch 16/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.4801\n",
      "Epoch 17/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.4624\n",
      "Epoch 18/32\n",
      "1250/1250 [==============================] - 87s 69ms/step - loss: 0.4467\n",
      "Epoch 19/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.4317\n",
      "Epoch 20/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.4181\n",
      "Epoch 21/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.4060\n",
      "Epoch 22/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3966\n",
      "Epoch 23/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3867\n",
      "Epoch 24/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3810\n",
      "Epoch 25/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3743\n",
      "Epoch 26/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3675\n",
      "Epoch 27/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3621\n",
      "Epoch 28/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3563\n",
      "Epoch 29/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3524\n",
      "Epoch 30/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3484\n",
      "Epoch 31/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3447\n",
      "Epoch 32/32\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22ca41c2370>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_model.fit(X_train, y_train.reshape(out_shape), epochs = 32, batch_size=64, validation_batch_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff2f3088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"s2sa\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     multiple                  4724736   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               multiple                  2099200   \n",
      "                                                                 \n",
      " attention (Attention)       multiple                  525825    \n",
      "                                                                 \n",
      " repeat_vector_3 (RepeatVect  multiple                 0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               multiple                  3147776   \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  8540424   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,037,961\n",
      "Trainable params: 19,037,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attention_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5fb1503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2500/2500 [==============================] - 180s 66ms/step - loss: 0.3982\n",
      "Epoch 2/16\n",
      "2500/2500 [==============================] - 166s 66ms/step - loss: 0.4266\n",
      "Epoch 3/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4347\n",
      "Epoch 4/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4392\n",
      "Epoch 5/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4381\n",
      "Epoch 6/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4373\n",
      "Epoch 7/16\n",
      "2500/2500 [==============================] - 164s 66ms/step - loss: 0.4364\n",
      "Epoch 8/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4343\n",
      "Epoch 9/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4317\n",
      "Epoch 10/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4283\n",
      "Epoch 11/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4237\n",
      "Epoch 12/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4196\n",
      "Epoch 13/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4153\n",
      "Epoch 14/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4094\n",
      "Epoch 15/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.4032\n",
      "Epoch 16/16\n",
      "2500/2500 [==============================] - 165s 66ms/step - loss: 0.3971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22cc9484760>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_model.fit(X_train, y_train.reshape(out_shape), epochs = 16, batch_size=32, validation_batch_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc5efd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "5000/5000 [==============================] - 309s 62ms/step - loss: 0.4660\n",
      "Epoch 2/8\n",
      "5000/5000 [==============================] - 309s 62ms/step - loss: 0.5143\n",
      "Epoch 3/8\n",
      "5000/5000 [==============================] - 309s 62ms/step - loss: 0.5248\n",
      "Epoch 4/8\n",
      "5000/5000 [==============================] - 309s 62ms/step - loss: 0.5248\n",
      "Epoch 5/8\n",
      "5000/5000 [==============================] - 310s 62ms/step - loss: 0.5219\n",
      "Epoch 6/8\n",
      "5000/5000 [==============================] - 310s 62ms/step - loss: 0.5139\n",
      "Epoch 7/8\n",
      "5000/5000 [==============================] - 312s 62ms/step - loss: 0.5098\n",
      "Epoch 8/8\n",
      "5000/5000 [==============================] - 314s 63ms/step - loss: 0.5031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d283a6850>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_model.fit(X_train, y_train.reshape(out_shape), epochs = 8, batch_size=16, validation_batch_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4e69b",
   "metadata": {},
   "source": [
    "(for 25 epochs)\n",
    "### Final Loss : 0.1697\n",
    "### Training Time : 5 hours, 45 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c521501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, dense_3_layer_call_fn, dense_3_layer_call_and_return_conditional_losses, dense_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2sa_v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2sa_v2\\assets\n",
      "WARNING:absl:<__main__.Attention object at 0x000002293160D190> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# attention_model.save(\"s2sa.keras\")\n",
    "attention_model.save(\"s2sa_v2\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304548ed",
   "metadata": {},
   "source": [
    "## Translation comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4809ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model1, model2, batch_size, size=len(X_test),):\n",
    "    p = np.argmax(model1.predict(X_test[:size], batch_size=batch_size), axis=-1)\n",
    "    q = np.argmax(model2.predict(X_test[:size], batch_size=batch_size), axis=-1)\n",
    "    real = e_tokenizer.sequences_to_texts(X_test[:size])\n",
    "    pred1 = d_tokenizer.sequences_to_texts(p)\n",
    "    pred2 = d_tokenizer.sequences_to_texts(q)\n",
    "    trans = d_tokenizer.sequences_to_texts(y_test[:size])\n",
    "    d = {'Eng': real,'Deu': trans, 'Base_Model': pred1, 'Attention_Model': pred2}\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1c05c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {'S2S':S2S}\n",
    "seq2seq = keras.models.load_model('s2s_model_tf_v2', custom_objects=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e013242",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = {'S2SA':S2SA}\n",
    "attention_model = keras.models.load_model('s2sa_v2', custom_objects=sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61f898fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31cf7939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 4ms/step\n",
      "400/400 [==============================] - 9s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eng</th>\n",
       "      <th>Deu</th>\n",
       "      <th>Base_Model</th>\n",
       "      <th>Attention_Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>she decided to marry tom</td>\n",
       "      <td>sie hat sich entschieden tom zu heiraten</td>\n",
       "      <td>sie entschied sich tom zu heiraten</td>\n",
       "      <td>sie entschied tom tom heiraten heiraten heiraten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do not read while walking</td>\n",
       "      <td>lies nicht im gehen</td>\n",
       "      <td>geht es nicht darüber</td>\n",
       "      <td>kann nicht französisch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which ones mine</td>\n",
       "      <td>welche ist meine</td>\n",
       "      <td>welches ist meiner</td>\n",
       "      <td>welcher ist toms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this knife is very sharp</td>\n",
       "      <td>dieses messer ist sehr scharf</td>\n",
       "      <td>dieses gekauft ist sehr lebt</td>\n",
       "      <td>dieses messer ist sehr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that was just plain stupid</td>\n",
       "      <td>das war einfach nur dumm</td>\n",
       "      <td>das war einfach werde dumm dumm</td>\n",
       "      <td>das war einfach so und blöd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>theres still a lot left</td>\n",
       "      <td>es gibt noch immer eine menge zu tun</td>\n",
       "      <td>es ist noch viel</td>\n",
       "      <td>es ist noch viel viel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>please add up the numbers</td>\n",
       "      <td>bitte addiert die zahlen</td>\n",
       "      <td>bitte möglich bitte zahlen</td>\n",
       "      <td>bitte sie sie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>he is playing in his room</td>\n",
       "      <td>er spielt in seinem zimmer</td>\n",
       "      <td>er zimmer sein seinem zimmer zimmer</td>\n",
       "      <td>er spielt in seinem zimmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tom isnt your brother</td>\n",
       "      <td>tom ist nicht dein bruder</td>\n",
       "      <td>das ist nicht dein bruder</td>\n",
       "      <td>tom ist nicht euer bruder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>im not going to stop</td>\n",
       "      <td>ich werde nicht aufhören</td>\n",
       "      <td>ich werde nicht aufhören</td>\n",
       "      <td>ich werde nicht aufhören</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i cant stand kids</td>\n",
       "      <td>ich hasse kinder</td>\n",
       "      <td>ich kann nicht kinder nicht</td>\n",
       "      <td>ich kann keine kinder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i want to study math</td>\n",
       "      <td>ich möchte mathematik studieren</td>\n",
       "      <td>ich möchte mathematik studieren</td>\n",
       "      <td>ich möchte für studieren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tom is very arrogant</td>\n",
       "      <td>tom ist ziemlich arrogant</td>\n",
       "      <td>tom ist sehr arrogant</td>\n",
       "      <td>tom ist sehr arrogant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i rubbed my feet</td>\n",
       "      <td>ich rieb meine füße</td>\n",
       "      <td>ich haben mir ihre füße</td>\n",
       "      <td>ich mir meine meine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>can we fish there</td>\n",
       "      <td>kann man hier angeln</td>\n",
       "      <td>können wir dort berühmt</td>\n",
       "      <td>können wir dort schrieb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>do you come here every day</td>\n",
       "      <td>kommst du hier jeden tag her</td>\n",
       "      <td>kommt du jeden tag</td>\n",
       "      <td>machst du jeden tag gekommen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>go away</td>\n",
       "      <td>verzieh dich</td>\n",
       "      <td>verdufte dich</td>\n",
       "      <td>mach dich weg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>they cheat</td>\n",
       "      <td>sie mogeln</td>\n",
       "      <td>sie damit</td>\n",
       "      <td>sie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>can you order it for me</td>\n",
       "      <td>können sie ihn mir bestellen</td>\n",
       "      <td>können sie sie mir bestellen</td>\n",
       "      <td>können sie es mir bestellen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tom put on some clothes</td>\n",
       "      <td>tom zog sich an</td>\n",
       "      <td>tom hat sich angezogen</td>\n",
       "      <td>tom hat etwas auf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Eng                                       Deu  \\\n",
       "0     she decided to marry tom  sie hat sich entschieden tom zu heiraten   \n",
       "1    do not read while walking                       lies nicht im gehen   \n",
       "2              which ones mine                          welche ist meine   \n",
       "3     this knife is very sharp             dieses messer ist sehr scharf   \n",
       "4   that was just plain stupid                  das war einfach nur dumm   \n",
       "5      theres still a lot left      es gibt noch immer eine menge zu tun   \n",
       "6    please add up the numbers                  bitte addiert die zahlen   \n",
       "7    he is playing in his room                er spielt in seinem zimmer   \n",
       "8        tom isnt your brother                 tom ist nicht dein bruder   \n",
       "9         im not going to stop                  ich werde nicht aufhören   \n",
       "10           i cant stand kids                          ich hasse kinder   \n",
       "11        i want to study math           ich möchte mathematik studieren   \n",
       "12        tom is very arrogant                 tom ist ziemlich arrogant   \n",
       "13            i rubbed my feet                       ich rieb meine füße   \n",
       "14           can we fish there                      kann man hier angeln   \n",
       "15  do you come here every day              kommst du hier jeden tag her   \n",
       "16                     go away                              verzieh dich   \n",
       "17                  they cheat                                sie mogeln   \n",
       "18     can you order it for me              können sie ihn mir bestellen   \n",
       "19     tom put on some clothes                           tom zog sich an   \n",
       "\n",
       "                             Base_Model  \\\n",
       "0    sie entschied sich tom zu heiraten   \n",
       "1                 geht es nicht darüber   \n",
       "2                    welches ist meiner   \n",
       "3          dieses gekauft ist sehr lebt   \n",
       "4       das war einfach werde dumm dumm   \n",
       "5                      es ist noch viel   \n",
       "6            bitte möglich bitte zahlen   \n",
       "7   er zimmer sein seinem zimmer zimmer   \n",
       "8             das ist nicht dein bruder   \n",
       "9              ich werde nicht aufhören   \n",
       "10          ich kann nicht kinder nicht   \n",
       "11      ich möchte mathematik studieren   \n",
       "12                tom ist sehr arrogant   \n",
       "13              ich haben mir ihre füße   \n",
       "14              können wir dort berühmt   \n",
       "15                   kommt du jeden tag   \n",
       "16                        verdufte dich   \n",
       "17                            sie damit   \n",
       "18         können sie sie mir bestellen   \n",
       "19               tom hat sich angezogen   \n",
       "\n",
       "                                     Attention_Model  \n",
       "0   sie entschied tom tom heiraten heiraten heiraten  \n",
       "1                             kann nicht französisch  \n",
       "2                                   welcher ist toms  \n",
       "3                             dieses messer ist sehr  \n",
       "4                        das war einfach so und blöd  \n",
       "5                              es ist noch viel viel  \n",
       "6                                      bitte sie sie  \n",
       "7                         er spielt in seinem zimmer  \n",
       "8                          tom ist nicht euer bruder  \n",
       "9                           ich werde nicht aufhören  \n",
       "10                             ich kann keine kinder  \n",
       "11                          ich möchte für studieren  \n",
       "12                             tom ist sehr arrogant  \n",
       "13                               ich mir meine meine  \n",
       "14                           können wir dort schrieb  \n",
       "15                      machst du jeden tag gekommen  \n",
       "16                                     mach dich weg  \n",
       "17                                               sie  \n",
       "18                       können sie es mir bestellen  \n",
       "19                                 tom hat etwas auf  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = test(seq2seq,attention_model, 10,4000)\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b74e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(model1, model2, batch_size, size=len(X_test),):\n",
    "    p = np.argmax(model1.predict(X[:size], batch_size=batch_size), axis=-1)\n",
    "    q = np.argmax(model2.predict(X[:size], batch_size=batch_size), axis=-1)\n",
    "    real = e_tokenizer.sequences_to_texts(X[:size])\n",
    "    pred1 = d_tokenizer.sequences_to_texts(p)\n",
    "    pred2 = d_tokenizer.sequences_to_texts(q)\n",
    "    trans = d_tokenizer.sequences_to_texts(y[:size])\n",
    "    d = {'Eng': real,'Deu': trans, 'Base_Model': pred1, 'Attention_Model': pred2}\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0504a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 4ms/step\n",
      "400/400 [==============================] - 8s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "df_test = test1(seq2seq,attention_model, 10,4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d4240b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eng</th>\n",
       "      <th>Deu</th>\n",
       "      <th>Base_Model</th>\n",
       "      <th>Attention_Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>wake up</td>\n",
       "      <td>wachen sie auf</td>\n",
       "      <td>steh sie</td>\n",
       "      <td>wach sie auf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>wake up</td>\n",
       "      <td>wach auf</td>\n",
       "      <td>steh sie</td>\n",
       "      <td>wach sie auf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>wake up</td>\n",
       "      <td>wachen sie auf</td>\n",
       "      <td>steh sie</td>\n",
       "      <td>wach sie auf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>wash up</td>\n",
       "      <td>wasch dir die hände</td>\n",
       "      <td>wasch dir die hände</td>\n",
       "      <td>wasch dir das gesicht</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>wash up</td>\n",
       "      <td>wasch dir das gesicht</td>\n",
       "      <td>wasch dir die hände</td>\n",
       "      <td>wasch dir das gesicht</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>we lost</td>\n",
       "      <td>wir haben verloren</td>\n",
       "      <td>wir haben haben</td>\n",
       "      <td>wir haben uns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>welcome</td>\n",
       "      <td>willkommen</td>\n",
       "      <td>willkommen</td>\n",
       "      <td>willkommen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>who ate</td>\n",
       "      <td>wer hat gegessen</td>\n",
       "      <td>wer hat gegessen</td>\n",
       "      <td>wer hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>who ate</td>\n",
       "      <td>wer aß</td>\n",
       "      <td>wer hat gegessen</td>\n",
       "      <td>wer hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>who ran</td>\n",
       "      <td>wer rannte</td>\n",
       "      <td>wer rannte</td>\n",
       "      <td>wer rannte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>who ran</td>\n",
       "      <td>wer ist gerannt</td>\n",
       "      <td>wer rannte</td>\n",
       "      <td>wer rannte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>who won</td>\n",
       "      <td>wer hat gewonnen</td>\n",
       "      <td>wer hat gewonnen</td>\n",
       "      <td>wer hat gewonnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>you run</td>\n",
       "      <td>du läufst</td>\n",
       "      <td>du laufen</td>\n",
       "      <td>sie laufen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>you run</td>\n",
       "      <td>sie laufen</td>\n",
       "      <td>du laufen</td>\n",
       "      <td>sie laufen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>you won</td>\n",
       "      <td>du hast gewonnen</td>\n",
       "      <td>du hast gewonnen</td>\n",
       "      <td>du hast gewonnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>all rise</td>\n",
       "      <td>alle aufstehen</td>\n",
       "      <td>alle aufstehen ist</td>\n",
       "      <td>alle aufstehen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>am i fat</td>\n",
       "      <td>bin ich dick</td>\n",
       "      <td>bin ich dick</td>\n",
       "      <td>bin ich dick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ask them</td>\n",
       "      <td>frag sie</td>\n",
       "      <td>frag sie</td>\n",
       "      <td>frag sie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>back off</td>\n",
       "      <td>komm nicht näher</td>\n",
       "      <td>komm wieder</td>\n",
       "      <td>komm zurück näher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>be a man</td>\n",
       "      <td>sei ein mann</td>\n",
       "      <td>sei ein mann mann</td>\n",
       "      <td>sei ein mann</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Eng                    Deu           Base_Model  \\\n",
       "290   wake up         wachen sie auf             steh sie   \n",
       "291   wake up               wach auf             steh sie   \n",
       "292   wake up         wachen sie auf             steh sie   \n",
       "293   wash up    wasch dir die hände  wasch dir die hände   \n",
       "294   wash up  wasch dir das gesicht  wasch dir die hände   \n",
       "295   we lost     wir haben verloren      wir haben haben   \n",
       "296   welcome             willkommen           willkommen   \n",
       "297   who ate       wer hat gegessen     wer hat gegessen   \n",
       "298   who ate                 wer aß     wer hat gegessen   \n",
       "299   who ran             wer rannte           wer rannte   \n",
       "300   who ran        wer ist gerannt           wer rannte   \n",
       "301   who won       wer hat gewonnen     wer hat gewonnen   \n",
       "302   you run              du läufst            du laufen   \n",
       "303   you run             sie laufen            du laufen   \n",
       "304   you won       du hast gewonnen     du hast gewonnen   \n",
       "305  all rise         alle aufstehen   alle aufstehen ist   \n",
       "306  am i fat           bin ich dick         bin ich dick   \n",
       "307  ask them               frag sie             frag sie   \n",
       "308  back off       komm nicht näher          komm wieder   \n",
       "309  be a man           sei ein mann    sei ein mann mann   \n",
       "\n",
       "           Attention_Model  \n",
       "290           wach sie auf  \n",
       "291           wach sie auf  \n",
       "292           wach sie auf  \n",
       "293  wasch dir das gesicht  \n",
       "294  wasch dir das gesicht  \n",
       "295          wir haben uns  \n",
       "296             willkommen  \n",
       "297                wer hat  \n",
       "298                wer hat  \n",
       "299             wer rannte  \n",
       "300             wer rannte  \n",
       "301       wer hat gewonnen  \n",
       "302             sie laufen  \n",
       "303             sie laufen  \n",
       "304       du hast gewonnen  \n",
       "305         alle aufstehen  \n",
       "306           bin ich dick  \n",
       "307               frag sie  \n",
       "308      komm zurück näher  \n",
       "309           sei ein mann  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[290:310]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee1940",
   "metadata": {},
   "source": [
    "## Amount of exact translations by each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1675736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1506"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test['Deu'] == df_test['Base_Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abefc664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1462"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test['Deu'] == df_test['Attention_Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e764717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ccdb3f5",
   "metadata": {},
   "source": [
    "# Questions and Answers\n",
    "\n",
    "How does the attention mechanism improve upon the basic Seq2Seq model, and which type of attention is implemented in this notebook?\n",
    "\n",
    "***Since the encoder network comprises the data into a single vector and is challenged by long input sequences, the implemented Bahdanau attention helps solve this by learning from different parts of the input. This improves seq2seq's ability to handle longer text.***\n",
    "\n",
    "What preprocessing steps are applied to the dataset, and why are they critical for training a sequence-to-sequence model?\n",
    "\n",
    "***The preprocessing pipeline cleans the data by taking an alphanumeric text, and lowercasing all texts for consistency. It was also tokenized to map each unique word to a unique numerical index so that machines can understand it. The vocabulary size and maximum length was calculated, and then the texts per data point was padded to ensure that the inputs are of the same length, which is needed for sequential deep learning models.***\n",
    "\n",
    "Explain how teacher forcing is applied in the decoder training process and its impact on convergence and performance.\n",
    "\n",
    "***Teacher forcing gives the true previous target to the decoder during training instead of its own prediction for a faster and more stable convergence. However, this approach can cause train-inference mismatch or exposure bias, where small mistakes compound into larger errors.***\n",
    "\n",
    "How are attention weights computed and integrated into the decoder’s hidden state during prediction?\n",
    "\n",
    "***Attention weights are computed by comparing the current decoder state with all encoder outputs to produce normalized softmax alignment scores. The weights indicate which part of the sequence is most relevant at each decoding step. A weighted sum of the encoder outputs is then formed (context vector) which is concatenated with the decoder input and passed through the LSTM, allowing the decoder to make predictions while focusing on the most important parts of the source.***\n",
    "\n",
    "What limitations can you observe in this implementation, and what modifications (e.g., bidirectional encoder, different attention type, transformer) might improve its performance?\n",
    "\n",
    "***The current implementation is very limited by the unidirectional nature of the LSTM encoder and reliance on sequential recurrence. Utilizing a bidirectional encoder along with attention would significantly improve performance for longer, more complex text sources.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb00ff",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "***In conclusion, this activity guided and allowed me to explore base sequence to sequence models and a more complex implementation with attention. The dataset was quite large, I could not finish it all in one sitting. The preprocessing done was textbook definition of those needed for a sequential model: removal of punctuations, lowercase letters, and padding for uniformity of length. Interestingly, the addition of attention showed visible improvement from the base Seq2Seq model. However, adding additional training epochs with lower batch size seemed to cause the model loss to increase, yielding worse performance than the initial models before experimentation. It would be interesting to see how more complex configuration such as bidirectional LSTM or other implementations of attention mechanisms could improve and increase performance on the given dataset.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c566c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1118439,
     "sourceId": 1878727,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 148465,
     "modelInstanceId": 125488,
     "sourceId": 147888,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24924.233277,
   "end_time": "2024-10-28T15:37:22.464444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-28T08:41:58.231167",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
